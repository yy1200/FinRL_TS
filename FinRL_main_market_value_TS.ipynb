{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AI4Finance-Foundation/FinRL/blob/master/FinRL_PortfolioAllocation_NeurIPS_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNmvYN9YbU4B"
   },
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ntfTb0e2bU4C",
    "outputId": "6dd3eee3-d5b4-4a17-f52f-61a3953092ef",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import math\n",
    "from datetime import datetime\n",
    "from Data_processor import YahooFinanceProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slBria_QbU4F"
   },
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 設定資料日期範圍\n",
    "# HISTORICAL_START_DATE = \"2012-06-01\"\n",
    "# HISTORICAL_END_DATE = \"2018-12-31\"\n",
    "\n",
    "# TRAIN_START_DATE = \"2012-06-01\"\n",
    "# TRAIN_END_DATE = \"2018-06-30\"\n",
    "\n",
    "# FIX_START_DATE = \"2018-01-01\"\n",
    "# FIX_END_DATE = \"2019-06-30\"\n",
    "\n",
    "# VALIDATION_START_DATE = \"2019-01-01\"\n",
    "# VALIDATION_END_DATE = \"2019-12-31\"\n",
    "\n",
    "# TEST_START_DATE = \"2020-01-01\"\n",
    "# TEST_END_DATE = \"2020-06-30\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WEwzMkFHbU4G",
    "outputId": "4745f4cc-3e6b-483f-92dc-a3ecae51d69c",
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 資料下載與前處理(只包含training範圍的資料)\n",
    "\n",
    "# # 股票代碼與技術指標的list\n",
    "# # 資料下載與前處理(只包含training範圍的資料)\n",
    "\n",
    "# # 股票代碼與技術指標的list\n",
    "# ticker_list = ['AAPL','MSFT','AMZN','TSLA','GOOGL','NVDA','BRK-A','META','UNH','JNJ ']\n",
    "# tech_list = []\n",
    "\n",
    "# # # 技術指標與VIX的開關，如果不想加就設定為False\n",
    "# if_vix = False\n",
    "# if_tech = False\n",
    "\n",
    "# days = [1, 5, 20, 60, 125, 250]\n",
    "\n",
    "# # # 下載並清洗資料\n",
    "# DP = YahooFinanceProcessor()\n",
    "# historical_data = DP.download_data(start_date = HISTORICAL_START_DATE,end_date = HISTORICAL_END_DATE,ticker_list = ticker_list, time_interval='1D')\n",
    "# historical_data = DP.clean_data(historical_data)\n",
    "# train_data = DP.download_data(start_date = TRAIN_START_DATE,end_date = TRAIN_END_DATE,ticker_list = ticker_list, time_interval='1D')\n",
    "# train_data = DP.clean_data(train_data)\n",
    "# fix_data = DP.download_data(start_date = FIX_START_DATE,end_date = FIX_END_DATE,ticker_list = ticker_list, time_interval='1D')\n",
    "# fix_data = DP.clean_data(fix_data)\n",
    "# validation_data = DP.download_data(start_date = VALIDATION_START_DATE,end_date = VALIDATION_END_DATE,ticker_list = ticker_list, time_interval='1D')\n",
    "# validation_data = DP.clean_data(validation_data)\n",
    "# test_data = DP.download_data(start_date = TEST_START_DATE,end_date = TEST_END_DATE,ticker_list = ticker_list, time_interval='1D')\n",
    "# test_data = DP.clean_data(test_data)\n",
    "\n",
    "# # # DataFrame轉成np.array\n",
    "# price_array_historical, tech_array_train, turbulence_array_train = DP.df_to_array(historical_data,tech_list, if_vix, if_tech)\n",
    "# price_array_train, tech_array_train, turbulence_array_train = DP.df_to_array(train_data,tech_list, if_vix, if_tech)\n",
    "# price_array_fix, tech_array_train, turbulence_array_train = DP.df_to_array(fix_data,tech_list, if_vix, if_tech)\n",
    "# price_array_validation, tech_array_train, turbulence_array_train = DP.df_to_array(validation_data,tech_list, if_vix, if_tech)\n",
    "# price_array_test, tech_array_test, turbulence_array_test = DP.df_to_array(test_data,tech_list, if_vix, if_tech)\n",
    "\n",
    "# aug_state_array_train = np.zeros((0,0))\n",
    "# aug_state_array_test = np.zeros((0,0))\n",
    "\n",
    "# # 加入技術指標和恐慌指數(如果有的話)\n",
    "# # if if_tech:\n",
    "# #     tech_list = ['close_30_sma','close_60_sma','macd','boll_ub','boll_lb','dx_30','rsi_30']\n",
    "# #     train_data = DP.add_technical_indicator(train_data, tech_list)\n",
    "# #     test_data = DP.add_technical_indicator(test_data, tech_list)\n",
    "    \n",
    "# # if if_vix:\n",
    "# #     train_data = DP.add_vix(train_data)\n",
    "# #     test_data = DP.add_vix(test_data)\n",
    "    \n",
    "# #     print(all_env_price)\n",
    "# #     all_env_tech.append(tech_array_train[start_day:start_day+200, :])\n",
    "# #     print(all_env_tech)\n",
    "# #     all_env_turbulence.append(turbulence_array_train[start_day:start_day+200])\n",
    "# #     print(all_env_turbulence)\n",
    "   \n",
    "# # 加入額外的state資訊 ( RN embedding )\n",
    "\n",
    "# # aug_state_array = np.zeros((price_array.shape[0],11,8)) # (754,11,8)\n",
    "# # 印一下形狀\n",
    "# #print('=============')\n",
    "# #print('price_array_train: ',price_array_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #存csv\n",
    "# pd.DataFrame(price_array_historical).to_csv('./data/historical.csv',index=False)\n",
    "# pd.DataFrame(price_array_train).to_csv('./data/train.csv',index=False)\n",
    "# pd.DataFrame(price_array_fix).to_csv('./data/fix.csv',index=False)\n",
    "# pd.DataFrame(price_array_validation).to_csv('./data/validation.csv',index=False)\n",
    "# pd.DataFrame(price_array_test).to_csv('./data/test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#讀csv\n",
    "price_array_historical = pd.read_csv('./data/historical.csv').to_numpy()\n",
    "price_array_train = pd.read_csv('./data/train.csv').to_numpy()\n",
    "price_array_fix = pd.read_csv('./data/fix.csv').to_numpy()\n",
    "price_array_validation = pd.read_csv('./data/validation.csv').to_numpy()\n",
    "price_array_test = pd.read_csv('./data/test.csv').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定義\n",
    "ticker_list = ['AAPL','MSFT','AMZN','TSLA','GOOGL','NVDA','BRK-A','META','UNH','JNJ ']\n",
    "days = [1, 5, 20, 60, 125, 250]\n",
    "validation_day = price_array_validation.shape[0]\n",
    "aug_state_array_train = np.zeros((0,0))\n",
    "aug_state_array_test = np.zeros((0,0))\n",
    "tech_array_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#price_return = pd.DataFrame(price_array[self.number]).pct_change(1).fillna(0).values\n",
    "#print('self.price_return',self.price_return.shape)\n",
    "price_historical_return = pd.DataFrame(price_array_historical).pct_change(1).fillna(0).values\n",
    "price_train_return = pd.DataFrame(price_array_train).pct_change(1).fillna(0).values\n",
    "price_fix_return = pd.DataFrame(price_array_fix).pct_change(1).fillna(0).values\n",
    "price_validation_return = pd.DataFrame(price_array_validation).pct_change(1).fillna(0).values\n",
    "price_test_return = pd.DataFrame(price_array_test).pct_change(1).fillna(0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(price_array_train.shape)\n",
    "# print(price_array_fix.shape)\n",
    "# print(price_array_validation.shape)\n",
    "# print(price_array_historical.shape)\n",
    "# price_historical_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxQTNjpblAMN"
   },
   "source": [
    "## Initiate Agent & Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 金融交易環境for資產配置\n",
    "class StockPortfolioEnv:  \n",
    "    \n",
    "    # 初始化參數由env_args傳入\n",
    "    def __init__(self, \n",
    "                 price_array, #各公司的股價(調整後收盤價) = #all_env_price, #隨機取連續200天的價格\n",
    "#                  tech_array, #技術指標陣列 = #all_env_tech, #隨機取連續200天的7種技術指標\n",
    "#                  aug_state_array, #其他的state(embedding / 時間資訊 / 資產資訊) = #all_env_turbulence, #隨機取連續200天的vix                 \n",
    "                 env_name, #環境名稱\n",
    "                 lookback, #window size(每次交易看過去幾天) \n",
    "                 is_train\n",
    "                ):\n",
    "        \n",
    "        initial_capital = 100000\n",
    "        self.number = 0\n",
    "        self.is_train = is_train\n",
    "        self.lookback = lookback  # 預設看過去10天\n",
    "        self.initial_total_asset = initial_capital # 初始總資產(股票+現金)\n",
    "        self.initial_cash = initial_capital #初始現金\n",
    "        \n",
    "        self.trans_cost = 0.01 #交易手續費(買賣都一樣)\n",
    "        self.buy_cost_pct = self.trans_cost # buy手續費\n",
    "        self.sell_cost_pct = self.trans_cost # sell手續費\n",
    "        self.gamma = 1 # reward遞減係數\n",
    "        \n",
    "        # 載入放入state的陣列\n",
    "        self.time = lookback-1 #現在的時間點\n",
    "        self.cash = self.initial_cash\n",
    "        self.price_array = price_array # (100,82,10)\n",
    "#         print(price_array.shape)\n",
    "#         print(price_array)\n",
    "        self.current_price_array = self.price_array[self.number] # (82,10)\n",
    "#         print(price_array[self.number])\n",
    "        self.current_price = self.current_price_array[self.time] # (1 ,10)\n",
    "#         print(self.current_price_array[self.time])\n",
    "#         print(self.price_array.shape[2])\n",
    "        self.stock_num = self.price_array.shape[2] #有幾隻股票要交易\n",
    "        \n",
    "        self.max_step =  self.price_array.shape[1] - lookback -1 #每個episode最多走幾步(期初走到最後一天)\n",
    "        self.env_num = 1 #有幾個環境(預設為1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 價格轉成daily return \n",
    "        self.price_return = pd.DataFrame(self.current_price_array).pct_change(1).fillna(0).values # (82,10)\n",
    "        \n",
    "#         self.tech_array = tech_array \n",
    "#         self.aug_state_array = turbulence_array  \n",
    "        #self.price_array =  all_env_price\n",
    "        #self.tech_array = all_env_tech \n",
    "        #self.aug_state_array = all_env_turbulence   \n",
    "        \n",
    "        \n",
    "#         self.current_tech = self.tech_array[self.number][self.time] if self.tech_array.shape[0]>0 else self.tech_array\n",
    "#         self.current_aug_state = self.aug_state_array[self.time] if self.aug_state_array.shape[0]>0 else self.aug_state_array\n",
    "        self.portfolio = np.zeros(self.stock_num, dtype=np.float32) #現在手上的股票部位\n",
    "        \n",
    "\n",
    "#         print('self.price_return',self.price_return.shape)\n",
    "\n",
    "        \n",
    "        # 紀錄訓練的結果\n",
    "        self.history = {'cumu_return':[],'action':[]}\n",
    "\n",
    "        # 紀錄現有資產及報酬率\n",
    "#         print('self.current_price:',self.current_price)\n",
    "        self.total_asset = self.cash + (self.portfolio * self.current_price).sum()\n",
    "        self.episode_return = 0.0  \n",
    "        self.gamma_return = 0.0\n",
    "        \n",
    "\n",
    "        '''env information'''\n",
    "        self.env_name = env_name\n",
    "        self.state_dim = self.price_return.shape[1]*lookback #1 + (self.price_array.shape[1] + self.tech_array.shape[0])*lookback\n",
    "        self.action_dim = self.price_return.shape[1]+1 #加一維cash\n",
    "        self.mid_dim = 16 #過完CNN每家公司的embedding長度  \n",
    "        self.aug_state_dim = 1\n",
    "        #self.current_working_dir = current_working_dir\n",
    "        \n",
    "\n",
    "        self.if_discrete = False\n",
    "        self.target_return = 10\n",
    "        self.total_cumulative_return = []\n",
    "        \n",
    "    # softmax\n",
    "    def action_normalization(self, actions):\n",
    "        numerator = np.exp(actions)\n",
    "        denominator = np.sum(np.exp(actions))\n",
    "        softmax_output = numerator/denominator\n",
    "        \n",
    "        return softmax_output\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        self.time = self.lookback-1\n",
    "        self.current_price = self.current_price_array[self.time]\n",
    "        \n",
    "#         self.current_tech = self.tech_array[self.number][self.time] if self.tech_array.shape[0]>0 else self.tech_array\n",
    "#         self.current_aug_state = self.aug_state_array[self.number][self.time] if self.aug_state_array.shape[0]>0 else self.aug_state_array\n",
    "        self.cash = self.initial_cash  # reset()\n",
    "        self.portfolio = np.zeros(self.stock_num, dtype=np.float32) # 持有股數\n",
    "        self.total_asset = self.cash + np.sum(self.portfolio *  self.current_price) # 總資產價值\n",
    "        self.history = {'cumu_return':[],'action':[],'total_asset':[],'episode return':[]}\n",
    "        self.episode_return = 0.0\n",
    "        self.gamma_return = 0.0\n",
    "        state  = self.get_state()\n",
    "        \n",
    "        return state\n",
    "\n",
    "    \n",
    "    def step(self, actions) -> (np.ndarray, float, bool, None):\n",
    "        self.time += 1\n",
    "        #print('time:',self.time)\n",
    "        \"\"\"transaction\"\"\"\n",
    "        # 隔一天的weight，包含股票及現金的持有比例\n",
    "        action = self.action_normalization(actions)\n",
    "        action = action.reshape(-1)\n",
    "        \n",
    "        # 新的持有股數：現在的weight*前一天總資產價值/股價(無條件捨去)\n",
    "        new_portfolio = np.floor(action[:-1]*self.total_asset/self.price_array[self.number][self.time-1])\n",
    "#         print(new_portfolio)\n",
    "        # 前一天配置股票剩餘資金納入手上現金部位\n",
    "        self.cash = self.total_asset - sum(new_portfolio*self.price_array[self.number][self.time-1])\n",
    "        \n",
    "        # 計算手續費\n",
    "        portfolio_change = np.sum((new_portfolio-self.portfolio)*self.price_array[self.number][self.time-1])\n",
    "        trans_cost = portfolio_change*self.trans_cost\n",
    "        self.cash-=trans_cost\n",
    "        \n",
    "        # 計算新的資產價值\n",
    "        new_total_asset = np.sum(new_portfolio*self.price_array[self.number][self.time])+self.cash\n",
    "        #print('cash',self.cash)\n",
    "        \n",
    "        # 計算報酬率\n",
    "        portfolio_return = new_total_asset/self.total_asset\n",
    "        self.cumu_return = new_total_asset/self.initial_total_asset\n",
    "        reward = (portfolio_return-1)*100\n",
    "        \n",
    "        #更新資產狀態\n",
    "        self.total_asset = new_total_asset\n",
    "        self.portfolio = new_portfolio\n",
    "        \n",
    "        self.gamma_return = self.gamma_return * self.gamma + reward \n",
    "        self.cumu_return = self.total_asset / self.initial_cash\n",
    "        \n",
    "#         print('self.cumu_return',self.cumu_return)\n",
    "        self.history['cumu_return'].append(self.cumu_return)\n",
    "        self.history['action'].append(action)\n",
    "        self.history['total_asset'].append(self.total_asset)\n",
    "                \n",
    "        \"\"\"update time\"\"\"\n",
    "        done = self.time == self.max_step+self.lookback\n",
    "        state = self.get_state()\n",
    "        if done:\n",
    "#             print('self.max_step',self.max_step)\n",
    "            reward = self.gamma_return\n",
    "#             print('reward',reward)\n",
    "            self.episode_return = self.total_asset / self.initial_total_asset\n",
    "            \n",
    "#             print('.episode_return', self.episode_return)\n",
    "            self.number += 1\n",
    "            self.number = self.number % self.price_array.shape[0] \n",
    "#             print('self.number:', self.number)\n",
    "            \n",
    "            ''' \n",
    "            # 畫圖\n",
    "            plt.plot(self.history['cumu_return'])\n",
    "            plt.savefig('./train_history/cumulative_return/cumulative_plot'+datetime.now().strftime('%Y-%m-%d %H:%M:%S')+'.png')\n",
    "            plt.close()\n",
    "            '''\n",
    "            \n",
    "            # 每個episode交易紀錄存檔\n",
    "            if self.is_train:\n",
    "                pd.DataFrame(self.history['cumu_return']).to_csv('./train_history(TS0)/cumulative_return/cumu_return'\\\n",
    "                                                                 +datetime.now().strftime(\"%Y%m%d-%H%M%S\")+'.csv')\n",
    "                pd.DataFrame(self.history['action']).to_csv('./train_history(TS0)/action/action'\\\n",
    "                                                                 +datetime.now().strftime(\"%Y%m%d-%H%M%S\")+'.csv')\n",
    "            else:\n",
    "                pd.DataFrame(self.history['cumu_return']).to_csv('./test_history(TS0)/cumulative_return/cumu_return'\\\n",
    "                                                                 +datetime.now().strftime(\"%Y%m%d-%H%M%S\")+'.csv')\n",
    "                avg_pnl = np.load('./test_history(TS0)/avg_pnl.npy') \n",
    "                avg_pnl = np.append(avg_pnl,self.history['cumu_return'][-1])\n",
    "                np.save('./test_history(TS0)/avg_pnl.npy',avg_pnl)\n",
    "        return state, reward, done, None\n",
    "\n",
    "    def get_state(self):\n",
    "        # 從現在時間往前取n天(lookback)\n",
    "        state = self.price_return[self.time-self.lookback+1:self.time+1].reshape(-1)*100 #flatten\n",
    "#         if self.aug_state_array.shape[0]!=0: #如果有aug_state的話\n",
    "#             state = np.concatenate(state,self.aug_state_array[self.time])\n",
    "\n",
    "#         if self.tech_array.shape[0]!=0: #如果有tech_array的話\n",
    "#             for i in range(self.lookback):\n",
    "#                 tech_i = self.tech_array[self.time-i]\n",
    "#                 normalized_tech_i = tech_i * 2 ** -15\n",
    "#                 state = np.hstack((state, normalized_tech_i)).astype(np.float32) \n",
    "#         print('state shape',state.shape)    \n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清空資料夾\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"./\" + 'train_history(TS0)'):\n",
    "    os.makedirs(\"./\" + 'train_history(TS0)')\n",
    "shutil.rmtree('./train_history(TS0)')\n",
    "if not os.path.exists(\"./\" + 'train_history(TS0)/cumulative_return'):\n",
    "    os.makedirs(\"./\" + 'train_history(TS0)/cumulative_return')    \n",
    "if not os.path.exists(\"./\" + 'train_history(TS0)/action'):\n",
    "    os.makedirs(\"./\" + 'train_history(TS0)/action')\n",
    "\n",
    "if not os.path.exists(\"./\" + 'test_history(TS0)'):\n",
    "    os.makedirs(\"./\" + 'test_history(TS0)')\n",
    "shutil.rmtree('./test_history(TS0)')\n",
    "if not os.path.exists(\"./\" + 'test_history(TS0)/cumulative_return'):\n",
    "    os.makedirs(\"./\" + 'test_history(TS0)/cumulative_return')\n",
    "if not os.path.exists(\"./\" + 'PPO_TS_0'):\n",
    "    os.makedirs(\"./\" + 'PPO_TS_0')\n",
    "# shutil.rmtree('./PPO_TS_0')\n",
    "np.save('./test_history(TS0)/avg_pnl.npy',np.array([])) # 清空紀錄檔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate RL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "# teacher model\n",
    "class Sampling_module(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sampling_module, self).__init__()\n",
    "        self.layer_1 = nn.Linear(120, 256)\n",
    "        self.layer_2 = nn.Linear(256, 128)\n",
    "        self.layer_out = nn.Linear(128, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.layer_out(x)\n",
    "        return x\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "sampling_model = Sampling_module()\n",
    "sampling_optimizer = optim.Adam(sampling_model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for j in range(validation_day-rolling_days):\n",
    "#     all_env_historical = [] #過去60個值\n",
    "#     for a in range(1281):\n",
    "#         temp_60 = []\n",
    "#         for b in days:\n",
    "#             for c in range(len(ticker_list)):\n",
    "#                 temp_60.append(price_train_return[j+days[5]+a-b:j+days[5]+a,c].sum())\n",
    "#         all_env_historical.append(temp_60)\n",
    "#     all_env_fix = [] #現在60個值\n",
    "#     for d in days:\n",
    "#         for e in range(len(ticker_list)):\n",
    "#             all_env_fix.append(price_fix_return[j+days[5]-d:j+days[5],e].sum()) \n",
    "#     value120 = []\n",
    "#     for k in range(len(all_env_historical)):\n",
    "#         value120.append(np.concatenate([all_env_historical[k],all_env_fix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_state = torch.FloatTensor(np.array(value120))  # 計算當前與過去的120個值\n",
    "# sam_pred = sampling_model(current_state)\n",
    "# np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "# act_Softmax = nn.Sigmoid()\n",
    "# sam_Softmax = act_Softmax(sam_pred)\n",
    "# print(sam_Softmax.shape)\n",
    "# # print(sam_Softmax.sum())\n",
    "# # torch.random.manual_seed(i)\n",
    "        \n",
    "# #         print('sam_Softmax:',sam_Softmax[:100])\n",
    "# pos_idx = np.argsort(sam_Softmax.detach().numpy()[:,0])[-100:]\n",
    "#         # 計算要給的學生learn的100個環境的idx(pos_idx)，以及沒被取到的1181個idx(neg_index)\n",
    "# # pos_idx = torch.multinomial(sam_Softmax,100)\n",
    "# pos_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling_model = torch.load('./act/teacher0.pth')\n",
    "# a = sampling_model.state_dict()\n",
    "# sampling_model = torch.load('./act/teacher1.pth')\n",
    "# b = sampling_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a['layer_1.weight']-b['layer_1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(sampling_model,'teacher.pth')state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from torch.distributions import Categorical\n",
    "# for j in range(validation_day-rolling_days):\n",
    "#     all_env_historical = [] #過去60個值\n",
    "#     for a in range(1281):\n",
    "#         temp_60 = []\n",
    "#         for b in days:\n",
    "#             for c in range(len(ticker_list)):\n",
    "#                 temp_60.append(price_train_return[j+days[5]+a-b:j+days[5]+a,c].sum())\n",
    "#         all_env_historical.append(temp_60)\n",
    "#     all_env_fix = [] #現在60個值\n",
    "#     for d in days:\n",
    "#         for e in range(len(ticker_list)):\n",
    "#             all_env_fix.append(price_fix_return[j+days[5]-d:j+days[5],e].sum()) \n",
    "#     value120 = []\n",
    "#     for k in range(len(all_env_historical)):\n",
    "#         value120.append(np.concatenate([all_env_historical[k],all_env_fix]))\n",
    "#     action_item = []\n",
    "#     for f in range(1281):\n",
    "#         current_state = torch.Tensor(np.array(value120[f]))  # 計算當前與過去的120個值\n",
    "#         current_state = current_state.to(torch.float32)\n",
    "#         sam_pred = sampling_model(current_state)\n",
    "#         act_Softmax = nn.Softmax(dim=0)\n",
    "#         sam_Softmax = act_Softmax(sam_pred)\n",
    "#         sam_Softmax = sam_Softmax.view(1, -1)\n",
    "#         m = Categorical(sam_Softmax)\n",
    "#         action = m.sample()\n",
    "#         action_item.append(action.item())\n",
    "#     np.set_printoptions(threshold=np.inf)\n",
    "#     print(np.array(action_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.use('module://matplotlib_inline.backend_inline')\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "#!pip install torch\n",
    "'''import你要跑的演算法'''\n",
    "import os\n",
    "import shutil\n",
    "from elegantrl.agents.AgentPPO import AgentPPO \n",
    "from elegantrl.train.config import Arguments\n",
    "from elegantrl.train.run import train_and_evaluate,test_agent,train_and_evaluate_mp\n",
    "import multiprocessing as mp\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.distributions import Categorical\n",
    "from multiprocessing import Process, Pool\n",
    "\n",
    "env_func = StockPortfolioEnv\n",
    "env_args = {\n",
    "    'env_name':'portfolio_allocation',\n",
    "    'if_discrete' : False,\n",
    "    'target_return' : 1000,\n",
    "    'lookback':10,\n",
    "    'env_num':1,\n",
    "    'max_step':1000,\n",
    "    'state_dim':100,\n",
    "    'action_dim':11,\n",
    "}\n",
    "\n",
    "validation_env_args = env_args.copy()\n",
    "test_env_args = env_args.copy()\n",
    "\n",
    "epochs = 1000\n",
    "rolling_days = days[4]\n",
    "historical_day = price_array_train.shape[0]-days[5] #1281\n",
    "# TODO：train teacher & student\n",
    "R = []\n",
    "r = 0\n",
    "IDX = np.zeros(1281)\n",
    "for i in range(100):#teacher_training_epoch\n",
    "    for j in range(1): #validation_day-rolling_days\n",
    "        if i%10==0:\n",
    "            buffer = torch.empty(10)\n",
    "        prob = torch.empty(1281)\n",
    "        # copy pretrain好的actor讓student讀\n",
    "        src=r'./actor.pth'\n",
    "        des=r'./PPO_TS_0/actor.pth'\n",
    "        shutil.copy(src, des)\n",
    "        \n",
    "        \n",
    "        # 製作老師的state\n",
    "        all_env_historical = [] #過去60個值\n",
    "        for a in range(historical_day):\n",
    "            temp_60 = []\n",
    "            for b in days:\n",
    "                for c in range(len(ticker_list)):\n",
    "                    temp_60.append(np.prod(price_train_return[j+days[5]+a-b:j+days[5]+a,c]+1)-1)\n",
    "            all_env_historical.append(temp_60)\n",
    "        all_env_fix = [] #現在60個值\n",
    "        for d in days:\n",
    "            for e in range(len(ticker_list)):\n",
    "                all_env_fix.append(np.prod(price_fix_return[j+days[5]-d:j+days[5],e]+1)-1) \n",
    "        value120 = []\n",
    "        for k in range(len(all_env_historical)):\n",
    "            value120.append(np.concatenate([all_env_historical[k],all_env_fix]))\n",
    "\n",
    "        \n",
    "        # 老師打分數，取前100個高的\n",
    "        current_state = torch.FloatTensor(np.array(value120))  # 計算當前與過去的120個值\n",
    "        sam_pred = sampling_model(current_state)\n",
    "#         act_Softmax = nn.Softmax()\n",
    "#         sam_Softmax = act_Softmax(sam_pred)\n",
    "        torch.random.manual_seed(i)\n",
    "        \n",
    "        \n",
    "        \n",
    "        act_Softmax = nn.Softmax(dim=1)\n",
    "        sam_Softmax = act_Softmax(sam_pred)\n",
    "#         pos_idx = np.argsort(sam_Softmax.detach().numpy()[:,0])[-100:]\n",
    "#         print('sam_Softmax:',sam_Softmax[:100])\n",
    "        \n",
    "        # 計算要給的學生learn的100個環境的idx(pos_idx)，以及沒被取到的1181個idx(neg_index)\n",
    "        action_item = np.ones(historical_day)\n",
    "        pos_idx = sam_Softmax[:,0].multinomial(num_samples=100, replacement=False)\n",
    "        IDX[pos_idx]+=1\n",
    "        for g in pos_idx:\n",
    "            action_item[g] = 0\n",
    "        for h in range(historical_day):\n",
    "            prob[h] = torch.log(sam_Softmax[h][int(action_item[h])])\n",
    "#         neg_index = list(set(range(1281))-set(pos_idx))\n",
    "#         action = sam_Softmax[:,1]\n",
    "#         action = np.arange(1281)\n",
    "#         for i in range(1281):\n",
    "#             if i in pos_idx:\n",
    "#                 action[i] = sam_Softmax[i,0]\n",
    "#             else:\n",
    "#                 action[i] = sam_Softmax[i,1]\n",
    "#         pos_log_prob = torch.log(torch.Tensor(action))\n",
    "#         pos_log_prob = torch.log(sam_Softmax[pos_idx,0])\n",
    "#         neg_log_prob = torch.log(sam_Softmax[neg_index,1])\n",
    "        \n",
    "        \n",
    "#         print('pos_idx',pos_idx)\n",
    "        all_env_train = []\n",
    "        for f in range(len(pos_idx)): #100\n",
    "            all_env_train.append(price_array_historical[pos_idx[f]+days[5]:pos_idx[f]+days[4]+days[5], :])\n",
    "#         print(\"train\")\n",
    "        train_env_args = env_args.copy()\n",
    "        train_env_args.update({\n",
    "            'price_array' : np.array(all_env_train),\n",
    "            'is_train' : True\n",
    "        })\n",
    "        train_args = Arguments(AgentPPO, env_func=env_func, env_args=train_env_args,cwd = \"PPO_TS_0\")\n",
    "        train_and_evaluate(train_args)\n",
    "\n",
    "        fileDir = r\"PPO_TS_0\"\n",
    "        fileSxt = r\"actor\"\n",
    "        fileExt = r\".pth\"\n",
    "        direction = [_ for _ in os.listdir(fileDir) if _.startswith(fileSxt) and _.endswith(fileExt) ]\n",
    "        direction.sort()\n",
    "#         print(direction[-1])\n",
    "#         os.rename(f'./PPO_TS_0/{direction[-1]}',f'./PPO_TS_0/actor.pth')\n",
    "        shutil.move(f'./PPO_TS_0/{direction[-1]}', f'./PPO_TS_0/actor.pth')\n",
    "\n",
    "        direction = [_ for _ in os.listdir(fileDir) if _== 'act_optim.pth' or _== 'act_target.pth']\n",
    "        if direction:\n",
    "            for file in direction:\n",
    "                os.remove(f'./PPO_TS_0/{file}')\n",
    "        \n",
    "\n",
    "        # 丟到驗證環境得到給老師的reward\n",
    "        validation_env = price_array_validation[j:j+days[4]]\n",
    "        validation_env_args.update({\n",
    "            'price_array' : np.expand_dims(np.array(validation_env), axis=0),\n",
    "#             'tech_array':tech_array_test,\n",
    "#             'turbulence_array':turbulence_array_test,\n",
    "#             'aug_state_array' : aug_state_array_test,\n",
    "            'is_train' : False,\n",
    "        })\n",
    "#         print(\"validation\")\n",
    "        validation_args = Arguments(AgentPPO, env_func=env_func, env_args=validation_env_args,cwd = \"PPO_TS_0\")\n",
    "        test_agent(validation_args)\n",
    "        \n",
    "        reward = np.load('./test_history(TS0)/avg_pnl.npy')\n",
    "        R.append(reward)\n",
    "        np.save('./test_history(TS0)/avg_pnl.npy',np.array([])) # 清空紀錄檔\n",
    "        print(i)\n",
    "        \n",
    "        plt.title('Reward')\n",
    "        plt.plot(R)\n",
    "        plt.autoscale()\n",
    "        pylab.show()\n",
    "        plt.savefig('./Reward.png',bbox_inches='tight')\n",
    "        \n",
    "        plt.cla()\n",
    "        plt.title('Prob')\n",
    "        plt.plot(sam_Softmax[:,0].detach().numpy())\n",
    "        plt.autoscale()\n",
    "        pylab.show()\n",
    "        plt.savefig('./Prob.png',bbox_inches='tight')\n",
    "        \n",
    "        plt.cla()\n",
    "        plt.title('Idx')\n",
    "        plt.plot(IDX)\n",
    "        plt.autoscale()\n",
    "        plt.savefig('./Idx.png',bbox_inches='tight')\n",
    "        \n",
    "        plt.cla()\n",
    "        Prob = sam_Softmax[:,0].detach().numpy()\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        base = np.array([IDX.max()]*1281)\n",
    "        new = base+Prob\n",
    "        ax.plot(Prob,'r')\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(IDX)\n",
    "        ax.set_ylim(Prob.min(),Prob.max())\n",
    "        ax2.set_ylim(IDX.min()-1, IDX.max()+1)\n",
    "        plt.savefig('./Idx&Prob.png',bbox_inches='tight')\n",
    "        \n",
    "#         print('grad:',sampling_model.layer_1.weight.grad)\n",
    "        torch.set_grad_enabled(True)\n",
    "        \n",
    "        buffer[i%10] = (-1*prob*torch.FloatTensor((R[r]-np.mean(R)))).mean()\n",
    "        l=(-1*prob*torch.FloatTensor((R[r]-np.mean(R)))).mean()\n",
    "        print('prob',prob.grad_fn)\n",
    "        print('loss.grad_fn',l.grad_fn)\n",
    "        print('l ',l)\n",
    "#         pos_loss = (-1*pos_log_prob*torch.FloatTensor((R[r]-1.035))).mean()\n",
    "#         neg_loss = (-1*neg_log_prob*torch.FloatTensor((R[r]-np.mean(R)))).mean()\n",
    "        r += 1\n",
    "        if (i-9)%10==0:\n",
    "            print(buffer.sum())\n",
    "            buffer.sum().backward()\n",
    "#             print('grad:',sampling_model.layer_1.weight.grad)\n",
    "            print('buffer_grad',buffer.grad)\n",
    "            sampling_optimizer.step()\n",
    "            sampling_optimizer.zero_grad()\n",
    "            print(\"update\")\n",
    "#         pos_loss.backward()\n",
    "#         torch.autograd.set_detect_anomaly(True)\n",
    "#         if i%10==0:\n",
    "#             sampling_optimizer.zero_grad()\n",
    "#             sampling_optimizer.step()\n",
    "        \n",
    "#         print('pos_loss:',pos_loss.item())\n",
    "#         print('neg_loss:',neg_loss.item())\n",
    "        \n",
    "#         neg_loss.backward()\n",
    "#         sampling_optimizer.step()\n",
    "#         sampling_optimizer.zero_grad()\n",
    "        \n",
    "#         torch.save(sampling_model,f'./act/teacher{j}.pth')\n",
    "    \n",
    "    \n",
    "print('=====testing=====')\n",
    "\n",
    "# test_agent(test_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_grad = Variable(torch.Tensor(buffer),requires_grad=True)\n",
    "reward = np.load('./test_history(TS0)/avg_pnl.npy')\n",
    "print(\"reward:\",reward.shape)\n",
    "R.append(reward)\n",
    "print(\"R:\",R)\n",
    "np.save('./test_history(TS0)/avg_pnl.npy',np.array([])) # 清空紀錄檔\n",
    "print(\"R_mean:\",np.mean(R))\n",
    "neg_pred = -1 * (R[r]-np.mean(R)) * buffer_grad #更新的reward #R有正有負\n",
    "r += 1\n",
    "neg_reinforce_loss = torch.sum(neg_pred).mean()\n",
    "sampling_optimizer.zero_grad()\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "neg_reinforce_loss = Variable(neg_reinforce_loss,requires_grad=True)\n",
    "neg_reinforce_loss.backward(retain_graph=True)\n",
    "sampling_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd: PPO_TS_0\n",
      "_path: PPO_TS_0/actor.pth\n"
     ]
    }
   ],
   "source": [
    "from elegantrl.agents.AgentPPO import AgentPPO \n",
    "from elegantrl.train.config import Arguments\n",
    "from elegantrl.agents.AgentBase import AgentBase\n",
    "from elegantrl.train.run import train_and_evaluate,test_agent\n",
    "import importlib\n",
    "from elegantrl.agents import AgentBase as agentbase\n",
    "import elegantrl\n",
    "importlib.reload(elegantrl.agents)\n",
    "price_array_test = pd.read_csv('./data/test.csv').to_numpy()\n",
    "price_array_test = np.expand_dims(price_array_test,axis=0)\n",
    "test_env_args.update({\n",
    "    'price_array' : price_array_test,\n",
    "#     'tech_array':tech_array_test,\n",
    "#     'turbulence_array':turbulence_array_test,\n",
    "#     'aug_state_array' : aug_state_array_test,\n",
    "    'is_train' : False\n",
    "})\n",
    "# print(all_env100_test.shape)\n",
    "test_args = Arguments(AgentPPO, env_func=env_func, env_args=test_env_args,cwd = \"PPO_TS_0\")\n",
    "test_agent(test_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./actor.pth')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.remove('UBAH_test.csv')\n",
    "ubah = pd.DataFrame(price_array_test[0]).pct_change().fillna(0)+1\n",
    "ubah.cumprod().mean(axis=1).to_csv('UBAH_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFO42LcomPUT"
   },
   "source": [
    "### plot training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "name = []\n",
    "cumu_return = []\n",
    "count = 0\n",
    "for root,_,files in os.walk('./train_history/cumulative_return'):\n",
    "    for i in files:\n",
    "        if i.endswith('csv') and 'checkpoint' not in i:\n",
    "            name.append(i)   \n",
    "            \n",
    "name.sort()\n",
    "for i in name:\n",
    "    count+=1\n",
    "    tmp = pd.read_csv(os.path.join('./train_history/cumulative_return',i))\n",
    "    cumu_return.append(tmp['0'].values[-1])\n",
    "            \n",
    "print('total episode:',count)\n",
    "ma = pd.DataFrame(cumu_return).rolling(50).mean()\n",
    "plt.title('episode cumulative return in training period')\n",
    "plt.ylabel('cumu return')\n",
    "plt.xlabel('number of episode')\n",
    "plt.plot(cumu_return)\n",
    "plt.plot(ma)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root = './train_history/action/'\n",
    "action = os.listdir(root)[-1]\n",
    "df = pd.read_csv(os.path.join(root,action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. df是每個時間點的action(portfolio weight)\n",
    "2. price_array是每家公司的股價\n",
    "3. ticker list是每間公司的代號(只是用來顯示在圖片的title而已)\n",
    "'''\n",
    "\n",
    "%matplotlib inline\n",
    "for num,name in enumerate(ticker_list):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    fig.set_figheight(15)\n",
    "    fig.set_figwidth(20)\n",
    "    ax = fig.add_subplot(111)\n",
    "    Y = price_array_test[11:,num]\n",
    "    X = range(len(Y))\n",
    "    buy_sig = df.iloc[:,num+1].diff().fillna(0)>0.1  # 大於10%才有箭頭 \n",
    "    sell_sig = df.iloc[:,num+1].diff().fillna(0)<-0.1\n",
    "    ax.plot(X,Y)\n",
    "    for i in X:\n",
    "        if buy_sig[i]:\n",
    "            #print('buy')\n",
    "            ax.annotate(\"\", xy=(i, Y[i]-0.1),xytext=(i, Y[i]-0.3), arrowprops=dict(facecolor=\"g\", alpha=0.5, headlength=4, width=0.1)) \n",
    "        elif sell_sig[i]:\n",
    "            #print('sell')\n",
    "            ax.annotate(\"\", xy=(i, Y[i]+0.1),xytext=(i, Y[i]+0.3),arrowprops=dict(facecolor=\"r\", alpha=0.5, headlength=4, width=0.1)) \n",
    "    plt.title(name)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "a=random.sample(range(price_array_train.shape[0]), 50)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAvxipWFmUe8",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### BackTest\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "lvrqTro3lhAh",
    "a3Iuv554xYFH",
    "SPEXBcm-uBJo",
    "iidB5E27dfzh"
   ],
   "include_colab_link": true,
   "name": "FinRL_PortfolioAllocation_NeurIPS_2020.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py39-venv",
   "language": "python",
   "name": "py39-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
